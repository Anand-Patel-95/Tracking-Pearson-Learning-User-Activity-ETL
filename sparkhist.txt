messages = spark.read.format("kafka").option("kafka.bootstrap.servers", "kafka:29092").option("subscribe","userAct").option("startingOffsets", "earliest").option("endingOffsets", "latest").load() 
messages.printSchema()
messages.show()
messages_as_strings=messages.selectExpr("CAST(value AS STRING)")
messages_as_strings.show()
messages_as_strings.printSchema()
messages_as_strings.count()
messages_as_strings.select('value').take(1)
messages_as_strings.select('value').take(1)[0].value
import json
first_message=json.loads(messages_as_strings.select('value').take(1)[0].value)
first_message
print(json.dumps(first_message, indent=4, sort_keys=True))
first_message["exam_name"]
first_message["sequences"]["counts"]["total"]
exit()
