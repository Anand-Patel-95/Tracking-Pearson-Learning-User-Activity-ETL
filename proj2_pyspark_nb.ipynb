{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Pyspark for Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.19.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff229574ac8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports of libraries for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "import pprint\n",
    "\n",
    "p = pprint.PrettyPrinter(indent=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume data from our kafka topic userAct\n",
    "\n",
    "Read in data to `messages` spark dataframe. spark.read.format(\"kafka\") tells us we are reading in data from kafka. We specify in option our bootstrap servers and the kafka port number specified in our docker-compose file. We also specify that we want to subscribe to the topic userAct. Our startingOffsets and endingOffsets are earliest and latest to specify that we want to read from the beginning to the end of the entire data. Lastly, load() will load this data into the messages spark data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"userAct\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object `messages` is a dataframe. We cache the messages to speed up access since we will be using them frequently in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the schema of the messages data we just read in. See that there are key value pairs. Values are the item of interest for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the top 20 rows of the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+---------+------+--------------------+-------------+\n",
      "| key|               value|  topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     4|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     5|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     6|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     7|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     8|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|     9|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    10|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    11|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    12|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    13|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    14|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    15|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    16|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    17|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    18|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|userAct|        0|    19|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+-------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe `messages_as_strings` as messages but only selecting the `value` column casted as a string data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages_as_strings=messages.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages_as_strings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_as_strings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages_as_strings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.count()` method lets us count the number of rows in this df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_as_strings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Data: Selecting one value row\n",
    "\n",
    "from `messages_as_strings`, select the 'value' column. Take 1 entry, if this was 200 it would take 200 entries from the beginning. `[0]` specifies the index of the entry to extract. The `value` takes out the value of this row, or the dict object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_as_strings.select('value').take(1)[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses `json.loads` to pass the above output into a json object. We use `json.dumps` to nicely print the json so we can see the fields, even those nested inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"base_exam_id\": \"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\n",
      "    \"certification\": \"false\",\n",
      "    \"exam_name\": \"Normal Forms and All That Jazz Master Class\",\n",
      "    \"keen_created_at\": \"1516717442.735266\",\n",
      "    \"keen_id\": \"5a6745820eb8ab00016be1f1\",\n",
      "    \"keen_timestamp\": \"1516717442.735266\",\n",
      "    \"max_attempts\": \"1.0\",\n",
      "    \"sequences\": {\n",
      "        \"attempt\": 1,\n",
      "        \"counts\": {\n",
      "            \"all_correct\": false,\n",
      "            \"correct\": 2,\n",
      "            \"incomplete\": 1,\n",
      "            \"incorrect\": 1,\n",
      "            \"submitted\": 4,\n",
      "            \"total\": 4,\n",
      "            \"unanswered\": 0\n",
      "        },\n",
      "        \"id\": \"5b28a462-7a3b-42e0-b508-09f3906d1703\",\n",
      "        \"questions\": [\n",
      "            {\n",
      "                \"id\": \"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\n",
      "                \"options\": [\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:24.670Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\n",
      "                        \"submitted\": 1\n",
      "                    },\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:25.914Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"f2528210-35c3-4320-acf3-9056567ea19f\",\n",
      "                        \"submitted\": 1\n",
      "                    },\n",
      "                    {\n",
      "                        \"checked\": false,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"d1bf026f-554f-4543-bdd2-54dcf105b826\"\n",
      "                    }\n",
      "                ],\n",
      "                \"user_correct\": false,\n",
      "                \"user_incomplete\": true,\n",
      "                \"user_result\": \"missed_some\",\n",
      "                \"user_submitted\": true\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"bbed4358-999d-4462-9596-bad5173a6ecb\",\n",
      "                \"options\": [\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:30.116Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"id\": \"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\n",
      "                        \"submitted\": 1\n",
      "                    },\n",
      "                    {\n",
      "                        \"checked\": false,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:41.791Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\n",
      "                        \"submitted\": 1\n",
      "                    }\n",
      "                ],\n",
      "                \"user_correct\": false,\n",
      "                \"user_incomplete\": false,\n",
      "                \"user_result\": \"incorrect\",\n",
      "                \"user_submitted\": true\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"e6ad8644-96b1-4617-b37b-a263dded202c\",\n",
      "                \"options\": [\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:52.510Z\",\n",
      "                        \"checked\": false,\n",
      "                        \"id\": \"a9333679-de9d-41ff-bb3d-b239d6b95732\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"checked\": false,\n",
      "                        \"id\": \"85795acc-b4b1-4510-bd6e-41648a3553c9\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:54.223Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\n",
      "                        \"submitted\": 1\n",
      "                    },\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:23:53.862Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\n",
      "                        \"submitted\": 1\n",
      "                    }\n",
      "                ],\n",
      "                \"user_correct\": true,\n",
      "                \"user_incomplete\": false,\n",
      "                \"user_result\": \"correct\",\n",
      "                \"user_submitted\": true\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"95194331-ac43-454e-83de-ea8913067055\",\n",
      "                \"options\": [\n",
      "                    {\n",
      "                        \"checked\": false,\n",
      "                        \"id\": \"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"checked\": false,\n",
      "                        \"id\": \"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"checked\": false,\n",
      "                        \"id\": \"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"at\": \"2018-01-23T14:24:00.807Z\",\n",
      "                        \"checked\": true,\n",
      "                        \"correct\": true,\n",
      "                        \"id\": \"7f13df9c-fcbe-4424-914f-2206f106765c\",\n",
      "                        \"submitted\": 1\n",
      "                    }\n",
      "                ],\n",
      "                \"user_correct\": true,\n",
      "                \"user_incomplete\": false,\n",
      "                \"user_result\": \"correct\",\n",
      "                \"user_submitted\": true\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"started_at\": \"2018-01-23T14:23:19.082Z\",\n",
      "    \"user_exam_id\": \"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print json for first assessment to see nested schema\n",
    "\n",
    "first_message = json.loads(messages_as_strings.select('value').take(1)[0].value)\n",
    "print(json.dumps(first_message, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe/table from our raw data\n",
    "\n",
    "Create a new dataframe `assessments2` by taking `messages_as_strings` df from above, it's rdd map, and mapping each row through a function with the `.map` method. Our function is a lambda function that will take the value, make a json of the value, and pass the arguements of the json dictionary into a Spark DF `Row(...)`. Lastly we convert this whole object after the `map` applied into a Spark DF using `.toDF()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments2 = messages_as_strings.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# p.pprint(assessments2.show())\n",
    "assessments2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(assessments2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assessments2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL: Answering Questions\n",
    "\n",
    "Sequences is a nested json. Spark SQL can help up unpack data from this data frame. We need Spark SQL to answer business question on the dataframe and get better tables for answering the questions.\n",
    "\n",
    "First, create a Spark \"TempTable\" (aka \"View\"). This is so that we can run Spark SQL queries on our assessments2 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments2.registerTempTable('assessments_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           exam_name| as|\n",
      "+--------------------+---+\n",
      "|Normal Forms and ...|1.0|\n",
      "|Normal Forms and ...|1.0|\n",
      "|The Principles of...|1.0|\n",
      "|The Principles of...|1.0|\n",
      "|Introduction to B...|1.0|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, max_attempts as from assessments_tbl order by max_attempts desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) How many assessments are in the dataset?\n",
    "\n",
    "I count **3280 assessments**. Determined by counting up the rows in the dataset corresponding to assessments taken.\n",
    "\n",
    "Note that there is a limitation to this since keen_id would have been a unique key for each assessment, but counting unique keen_id's yields less than 3280 assessments. Which is to be believed? We have 38 assessments with not unique keen_id's. Were these assessments given non-unique keen_id's by accident, or is keen_id not a reliable unique primary key for assessments? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_assessments|\n",
      "+-----------------+\n",
      "|             3280|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by row count\n",
    "spark.sql(\"select count(*) as total_assessments from assessments_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_assessments|\n",
      "+-----------------+\n",
      "|             3242|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by keen id\n",
    "spark.sql(\"select count(distinct keen_id) as total_assessments from assessments_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) How many distinct exams were taken by users?\n",
    "\n",
    "There were **107 unique exams** taken by users in this dataset. We look at base_exam_id rather than exam_name since some exams could have the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|unique_exams|\n",
      "+------------+\n",
      "|         107|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct base_exam_id) as unique_exams from assessments_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) How many people took Learning Git?\n",
    "\n",
    "We need to create a table for exams and times taken by exam name. On this table, we would run the queries to answer this question. However, there is a problem with how to count the times taken for an exam. \n",
    "\n",
    "The user_exam_id should give a distinct user assessment attempt for an exam, so we should count that for grouped exams. However, this leads to fewer times taken than just counting the number of times an exam showed up for an assessment. We will assume that the number of assessments for an exam gives the number of people who took Learning Git. Therefore we see that **394 people took Learning Git**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           exam_name|times_taken|\n",
      "+--------------------+-----------+\n",
      "|        Learning Git|        394|\n",
      "|Introduction to P...|        162|\n",
      "|Intermediate Pyth...|        158|\n",
      "|Introduction to J...|        158|\n",
      "|Learning to Progr...|        128|\n",
      "|Introduction to M...|        119|\n",
      "|Software Architec...|        109|\n",
      "|Beginning C# Prog...|         95|\n",
      "|    Learning Eclipse|         85|\n",
      "|Learning Apache M...|         80|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# based on the number of assessments for this exam\n",
    "exams_taken_df = spark.sql(\"select exam_name, count(*) as times_taken from assessments_tbl group by exam_name order by times_taken desc limit 10\")\n",
    "exams_taken_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           exam_name|times_taken|\n",
      "+--------------------+-----------+\n",
      "|        Learning Git|        390|\n",
      "|Introduction to P...|        162|\n",
      "|Introduction to J...|        158|\n",
      "|Intermediate Pyth...|        156|\n",
      "|Learning to Progr...|        128|\n",
      "|Introduction to M...|        119|\n",
      "|Software Architec...|        109|\n",
      "|    Learning Eclipse|         85|\n",
      "|Beginning C# Prog...|         83|\n",
      "|Learning Apache M...|         80|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# based on the number of distinct user_exam_id for this exam\n",
    "spark.sql(\"select exam_name, count(distinct user_exam_id) as times_taken from assessments_tbl group by exam_name order by times_taken desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this exams_taken_df to a temp table for next queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exams_taken_df = spark.sql(\"select exam_name, count(*) as times_taken from assessments_tbl group by exam_name order by times_taken desc\")\n",
    "exams_taken_df.registerTempTable('exams_taken_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) What is the least common course taken? And the most common?\n",
    "Least common course taken: **Learning to Visualize Data with D3.js** with 1 assessment. Most common course taken: **Learning Git** with 394 assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|exam_name   |times_taken|\n",
      "+------------+-----------+\n",
      "|Learning Git|394        |\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, times_taken from exams_taken_tbl order by times_taken desc limit 1\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-----------+\n",
      "|exam_name                            |times_taken|\n",
      "+-------------------------------------+-----------+\n",
      "|Learning to Visualize Data with D3.js|1          |\n",
      "+-------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, times_taken from exams_taken_tbl order by times_taken limit 1\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Tables to HDFS\n",
    "\n",
    "Taking our Spark df, applying method to write it, specifying a parquet file for storage, and entering the path to where we want to store our data in HDFS and the name at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Table 1 for Q1), Q2), Q3)\n",
    "assessments2.write.parquet(\"/tmp/assessments_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Table 2 for Q4)\n",
    "exams_taken_df.write.parquet(\"/tmp/exams_taken_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Work: Making Tables from Nested Columns\n",
    "### Looking at Sequences\n",
    "\n",
    "Make table that combines exam name and info about score on the exam and number of questions.\n",
    "\n",
    "1-to-1 between row and extracted data from sequences, so use map instead of flatMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent_score_from_json_flatMap(row):\n",
    "    # grab the row for this exam\n",
    "    exam = json.loads(row.value)\n",
    "    \n",
    "    # check if keys of sequences contains counts\n",
    "    score_calc = -1 # default value\n",
    "    num_q_calc = -1 # default value\n",
    "    \n",
    "    # sequences must exist\n",
    "    if \"sequences\" in exam.keys():\n",
    "        # counts must exist\n",
    "        if \"counts\" in exam[\"sequences\"]:\n",
    "            # question data is not weird\n",
    "            if (\"correct\" in exam[\"sequences\"][\"counts\"]) and (\"total\" in exam[\"sequences\"][\"counts\"]) and (exam[\"sequences\"][\"counts\"][\"total\"] != 0):\n",
    "                score_calc = 100*exam[\"sequences\"][\"counts\"][\"correct\"]/exam[\"sequences\"][\"counts\"][\"total\"]\n",
    "                num_q_calc = len(exam[\"sequences\"][\"questions\"])                \n",
    "    \n",
    "    exam_details = {\"base_exam_id\": exam[\"base_exam_id\"],\n",
    "         \"exam_name\": exam[\"exam_name\"],\n",
    "         \"keen_id\": exam[\"keen_id\"],\n",
    "         \"score\": score_calc,\n",
    "         \"num_questions\": num_q_calc,\n",
    "         \"user_exam_id\": exam[\"user_exam_id\"]}\n",
    "    \n",
    "    return [Row(**exam_details)]\n",
    "\n",
    "def percent_score_from_json(row):\n",
    "    # grab the row for this exam\n",
    "    exam = json.loads(row.value)\n",
    "    \n",
    "    # check if keys of sequences contains counts\n",
    "    score_calc = -1 # default value\n",
    "    num_q_calc = -1 # default value\n",
    "    \n",
    "    # sequences must exist\n",
    "    if \"sequences\" in exam.keys():\n",
    "        # counts must exist\n",
    "        if \"counts\" in exam[\"sequences\"]:\n",
    "            # question data is not weird\n",
    "            if (\"correct\" in exam[\"sequences\"][\"counts\"]) and (\"total\" in exam[\"sequences\"][\"counts\"]) and (exam[\"sequences\"][\"counts\"][\"total\"] != 0):\n",
    "                score_calc = 100*exam[\"sequences\"][\"counts\"][\"correct\"]/exam[\"sequences\"][\"counts\"][\"total\"]\n",
    "                num_q_calc = len(exam[\"sequences\"][\"questions\"])                \n",
    "    \n",
    "    exam_details = {\"base_exam_id\": exam[\"base_exam_id\"],\n",
    "         \"exam_name\": exam[\"exam_name\"],\n",
    "         \"keen_id\": exam[\"keen_id\"],\n",
    "         \"score\": score_calc,\n",
    "         \"num_questions\": num_q_calc,\n",
    "         \"user_exam_id\": exam[\"user_exam_id\"]}\n",
    "    \n",
    "    return Row(**exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exams_and_scores = messages_as_strings.rdd.map(percent_score_from_json).toDF()\n",
    "# exams_and_scores = messages_as_strings.rdd.flatMap(percent_score_from_json).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- num_questions: long (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_and_scores.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+-----------------+--------------------+\n",
      "|        base_exam_id|           exam_name|             keen_id|num_questions|            score|        user_exam_id|\n",
      "+--------------------+--------------------+--------------------+-------------+-----------------+--------------------+\n",
      "|37f0a30a-7464-11e...|Normal Forms and ...|5a6745820eb8ab000...|            4|             50.0|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|Normal Forms and ...|5a674541ab6b0a000...|            4|             25.0|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|The Principles of...|5a67999d3ed3e3000...|            4|             75.0|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|The Principles of...|5a6799694fc7c7000...|            4|             50.0|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|Introduction to B...|5a6791e824fccd000...|            4|             75.0|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        Learning Git|5a67a0b6852c2a000...|            5|            100.0|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|Git Fundamentals ...|5a67b627cc80e6000...|            1|            100.0|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|Introduction to P...|5a67ac8cb0a5f4000...|            5|            100.0|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|Intermediate Pyth...|5a67a9ba060087000...|            4|            100.0|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|Introduction to P...|5a67ac54411aed000...|            5|              0.0|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|A Practical Intro...|5a67ad9b2ff312000...|            4|             75.0|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|Git Fundamentals ...|5a67b610baff90000...|            1|            100.0|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|Introduction to M...|5a67ac9837b82b000...|            6|66.66666666666667|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|   Python Epiphanies|5a67aaa4f21cc2000...|            6|66.66666666666667|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|Introduction to P...|5a67ac46f7bce8000...|            5|             80.0|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|Python Data Struc...|5a67aedaf34e85000...|            4|             75.0|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|Python Data Struc...|5a67aefef5e149000...|            4|             75.0|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|Working with Algo...|5a67ae3f0c5f48000...|            4|            100.0|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|Learning iPython ...|5a67ad579d5057000...|            4|             50.0|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|   Python Epiphanies|5a67aae6753fd6000...|            6|            100.0|d0f8249a-597e-4e1...|\n",
      "+--------------------+--------------------+--------------------+-------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exams_and_scores.show(20, False)\n",
    "exams_and_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out the nulls for score\n",
    "exams_and_scores = exams_and_scores.filter(\"score is not null\") # not null\n",
    "exams_and_scores = exams_and_scores.filter(\"score != -1\") # score not default val of -1\n",
    "exams_and_scores = exams_and_scores.filter(\"num_questions != -1\") # num_questions not default val of -1\n",
    "\n",
    "\n",
    "# can do it in spark sql, just save out results to spark df\n",
    "\n",
    "# google to see how to filter out for any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# register temp table to run Spark SQL queries on\n",
    "exams_and_scores.registerTempTable(\"examsScores_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of exams in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|total_num_exams|\n",
      "+---------------+\n",
      "|           3275|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count number of rows in table\n",
    "spark.sql(\"SELECT COUNT(*) as total_num_exams FROM examsScores_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           exam_name|times_taken|\n",
      "+--------------------+-----------+\n",
      "|Learning Data Mod...|          9|\n",
      "|Networking for Pe...|         15|\n",
      "|Introduction to J...|        158|\n",
      "|Learning Apache H...|         16|\n",
      "|Learning Spring P...|          2|\n",
      "|Learning iPython ...|         17|\n",
      "|Introduction to P...|        162|\n",
      "|Learning C# Best ...|         35|\n",
      "|Introduction to A...|         14|\n",
      "|A Practical Intro...|          9|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "spark.sql(\"select exam_name, count(*) as times_taken from examsScores_tbl group by exam_name limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5) Which exam had the most questions?\n",
    "\n",
    "The exam **Operating Red Hat Enterprise Linux Servers** has the most questions: 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------------+\n",
      "|exam_name                                 |num_questions|\n",
      "+------------------------------------------+-------------+\n",
      "|Operating Red Hat Enterprise Linux Servers|20           |\n",
      "|Great Bash                                |10           |\n",
      "|Learning Linux System Administration      |8            |\n",
      "|Learning to Program with R                |7            |\n",
      "|Introduction to Data Science with R       |7            |\n",
      "|Being a Better Introvert                  |7            |\n",
      "|Understanding the Grails 3 Domain Model   |7            |\n",
      "|What's New in JavaScript                  |7            |\n",
      "|Using Web Components                      |6            |\n",
      "|Arduino Prototyping Techniques            |6            |\n",
      "+------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(exam_name), num_questions from examsScores_tbl order by num_questions desc limit 10\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6) What is the average score on Learning Git? \n",
    "\n",
    "The average score on Learning Git is **67.61%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+\n",
      "|   exam_name|times_taken|avg_score|\n",
      "+------------+-----------+---------+\n",
      "|Learning Git|        394|    67.61|\n",
      "+------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(*) as times_taken, round(avg(score),2) as avg_score from examsScores_tbl group by exam_name having exam_name == 'Learning Git' order by times_taken desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Write to HDFS for table used to answer Q5), Q6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Table 2 for Q5), Q6)\n",
    "exams_and_scores.write.parquet(\"/tmp/exams_and_scores_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion of Spark Session. Return to `proj2_writeup.md`, section `Checking saved tables in HDFS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
